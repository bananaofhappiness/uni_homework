{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f9b19be-00ba-481b-8b19-0e6301a648cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Задача 1 (2 балла). \n",
    "\n",
    "Попробуем себя в решении задачи определения темы текста. Будем считать, что два текста похожи по теме, если у них больше общих слов (только не предлогов с союзами), чем у других текстов. У нашей программы для определения темы будет несколько готовых текстов (достаточно больших!) с уже известной темой в базе: выберите тексты (и темы) самостоятельно, 5-6 будет достаточно. \n",
    "\n",
    "Что должна делать программа? При запуске вы ей сообщаете название нового файла с текстом, который нужно классифицировать, она его открывает, обрабатывает и сравнивает с текстами в своей базе. С которым из текстов оказалось больше всего общих слов, того и тема! Очевидно, вам понадобится какие-то слова из текстов отбрасывать (подумайте, каким образом это сделать - здесь на самом деле несколько вариантов концепций), а еще лемматизировать или хотя бы использовать стемминг. \n",
    "\n",
    "Когда будете сдавать это задание, пожалуйста, пришлите и файлы с текстами. И имейте в виду, если тексты будут вставлены прямо в код и слишком короткие, я задачу засчитаю только вполовину. \n",
    "\n",
    "Для красоты можно 1) написать все в классах (вообще-то даже **нужно**) 2) использовать argparse (тогда код в .py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b788fe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from textsim import TextSimilarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc85b48",
   "metadata": {},
   "source": [
    "Я также использую тексты таких тем, на которых программа не обучалась."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffdfcd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анализация радиоэлектроника.txt\n",
      "Вероятная тема текста: базы данных.txt\n",
      "\n",
      "Анализация архитектура.txt\n",
      "Вероятная тема текста: экономика.txt\n",
      "\n",
      "Анализация документальная литература.txt\n",
      "Вероятная тема текста: научная фантастика.txt\n",
      "\n",
      "Анализация экономика.txt\n",
      "Вероятная тема текста: экономика.txt\n",
      "\n",
      "Анализация базы данных.txt\n",
      "Вероятная тема текста: базы данных.txt\n",
      "\n",
      "Анализация домашние животные.txt\n",
      "Вероятная тема текста: научная фантастика.txt\n",
      "\n",
      "Анализация научная фантастика.txt\n",
      "Вероятная тема текста: научная фантастика.txt\n",
      "\n",
      "Анализация военная документалистика.txt\n",
      "Вероятная тема текста: документальная литература.txt\n",
      "\n",
      "Анализация сад и огород.txt\n",
      "Вероятная тема текста: домашние животные.txt\n",
      "\n",
      "Анализация программирование.txt\n",
      "Вероятная тема текста: домашние животные.txt\n",
      "\n",
      "Анализация кулинария.txt\n",
      "Вероятная тема текста: кулинария.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_similarity = TextSimilarity()\n",
    "\n",
    "path = os.path.join(\"test_texts\", \"*\")\n",
    "\n",
    "for file in glob.glob(path):\n",
    "    file_name = os.path.basename(file)\n",
    "    with open(file) as file:\n",
    "        print(f\"Анализация {file_name}\")\n",
    "        print(f\"{text_similarity.get_theme(file.read())}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4654cd",
   "metadata": {},
   "source": [
    "Проблемы возникли только с документалистикой, но она и так довольная разная бывает."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072320b6-ffcc-460d-846d-9a3c02d32972",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Задача 2 (3 балла). \n",
    "\n",
    "Некоторые предлоги в русском языке могут управлять разными падежами (например, \"я еду в Лондон\" vs \"я живу в Лондоне\"). Давайте проанализируем эти предлоги и их падежи. Необходимо:\n",
    "\n",
    "- составить список таких предлогов (РГ-80 вам в помощь)\n",
    "- взять достаточно большой текст (можно большое художественное произведение)\n",
    "- сделать морфоразбор этого текста\n",
    "- Посчитать, как часто и какие падежи встречаются у слова, идущего после предлога.\n",
    "\n",
    "Примечания: во-первых, имейте в виду, что иногда после предлога могут идти самые неожиданные вещи: \"я что, должен ехать на, черт побери, северный полюс?\". Во-вторых, неплохо бы учитывать отсутствие пунктуации (конечно, в норме, как нам кажется, предлог обязательно требует зависимое, но! \"да иди ты на!\") Эти штуки можно отсеять, если просто учитывать только заранее определенные падежи, а не считать все, какие встретились (так и None можно огрести). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aacf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# р.п   без, для, до, из, из-за, из-под, меж, между, от, с, у\n",
    "# д.п   к, по\n",
    "# в.п   в, за, на, о, по, под, про, с, через\n",
    "# т.п   за, меж, между, над, перед, под, с\n",
    "# п.п   в, на, о, по, при\n",
    "# \n",
    "# меж   р.п, т.п\n",
    "# между р.п, т.п\n",
    "# с     р.п, в.п, т.п\n",
    "# по    д.п, в.п, п.п\n",
    "# в     в.п, п.п\n",
    "# за    в.п, т.п\n",
    "# на    в.п, п.п\n",
    "# о     в.п, п.п\n",
    "# под   в.п, т.п "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03f53bea-ad35-4869-841e-d3a492508c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    Doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5684881a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество Acc_all = 2990\n",
      "Количество Dat_all = 698\n",
      "Количество Gen_all = 289\n",
      "Количество Ins_all = 1455\n",
      "Количество Loc_all = 3314\n",
      "Количество Nom_all = 10\n",
      "Количество Par_all = 3\n",
      "Количество в_Acc = 1294\n",
      "Количество в_Ins = 1\n",
      "Количество в_Loc = 2091\n",
      "Количество в_Nom = 5\n",
      "Количество за_Acc = 188\n",
      "Количество за_Gen = 2\n",
      "Количество за_Ins = 269\n",
      "Количество за_Nom = 3\n",
      "Количество меж_Gen = 1\n",
      "Количество меж_Ins = 1\n",
      "Количество между_Gen = 15\n",
      "Количество между_Ins = 49\n",
      "Количество на_Acc = 1425\n",
      "Количество на_Gen = 3\n",
      "Количество на_Ins = 1\n",
      "Количество на_Loc = 881\n",
      "Количество о_Acc = 17\n",
      "Количество о_Ins = 1\n",
      "Количество о_Loc = 336\n",
      "Количество по_Acc = 15\n",
      "Количество по_Dat = 698\n",
      "Количество по_Loc = 6\n",
      "Количество под_Acc = 48\n",
      "Количество под_Ins = 119\n",
      "Количество с_Acc = 3\n",
      "Количество с_Gen = 268\n",
      "Количество с_Ins = 1014\n",
      "Количество с_Nom = 2\n",
      "Количество с_Par = 3\n"
     ]
    }
   ],
   "source": [
    "segmenter = Segmenter()\n",
    "embeding = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(embeding)\n",
    "\n",
    "with open(\"литература.txt\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "doc = Doc(text)\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "adp_list = [\"меж\", \"между\", \"с\", \"по\", \"в\", \"за\", \"на\", \"о\", \"под\"]\n",
    "\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)\n",
    "for sent in doc.sents:\n",
    "    iterator = iter(sent.morph.tokens)\n",
    "\n",
    "    for token in iterator:\n",
    "        # если токен — предлог из списка\n",
    "        # (token.pos == \"ADP\" не обязательно проверять, но на всякий случай можно)\n",
    "        if token.text.lower() in adp_list and token.pos == \"ADP\":\n",
    "            try:\n",
    "                # тогда смотрим на следующее слово\n",
    "                # и если оно есть и у него есть падеж\n",
    "                # добавляем в счетчик\n",
    "                next_token = next(iterator)\n",
    "                if \"Case\" in next_token.feats:\n",
    "                    adp_case = token.text.lower() + \"_\" + next_token.feats[\"Case\"]\n",
    "                    counter[adp_case] += 1\n",
    "\n",
    "                    case_ = next_token.feats[\"Case\"] + \"_all\"\n",
    "                    counter[case_] += 1\n",
    "            except StopIteration:\n",
    "                ...\n",
    "    \n",
    "for k, v in sorted(counter.items()):\n",
    "    print(f\"Количество {k} = {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ab1fb8",
   "metadata": {},
   "source": [
    "Забавно, тут есть предлог о с инструменталисом. На самом деле это \"о злом племени\". ну тут уже не моя ошибка, можно просто выкинуть элементы, которые встречаются менее 10 раз."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e5bf20-f019-44bc-adaf-a9702898011e",
   "metadata": {},
   "source": [
    "#### Задача 3. \n",
    "\n",
    "Представим, что у вас есть файл с разборами conllu (можете взять любой, например, [тут](https://github.com/dialogue-evaluation/GramEval2020)). Нужно просмотреть все примеры предложений с тегом dislocated и тегом discourse: напишите скрипт, который будет читать файл, находить все такие предложения и печатать: 1) сам текст предложения 2) слово, имеющее искомый тег. Если тег не был найден в файле, нужно об этом сообщить. Постарайтесь оформить вывод таким образом, чтобы это было удобно читать. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d7a19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse_incr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a88c258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_sentence(sentence):\n",
    "    collected_sentence = \"\"\n",
    "    for token in sentence:\n",
    "        collected_sentence += token[\"form\"]\n",
    "        if token[\"misc\"] is not None:\n",
    "            continue\n",
    "        collected_sentence += \" \"\n",
    "    return collected_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fbbf095-7332-41cb-b5b3-f00d44d386c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сын чиновника, Покровский родился в 1808 г. и воспитывался в Казанской Гимназии, в которой окончил курс в 1820 г.; в 1823 г. он поступил в Казанский Университет (28-го июня), из которого вышел 28-го июня 1826 г. со степенью кандидата, после чего определился учителем средних классов Казанской Гимназии, с прикомандированием к Педагогическому Институту, состоявшему при Университете. \n",
      "Слово с тегом dislocated: Сын\n",
      "Поп Григорий показывал: ``какая де на нем, архиерее, скорбь была, про тое де скорбь он, поп Григорий, не скажет, потому что за клятвою его, архиерейскою&#39;&#39;. \n",
      "Слово с тегом discourse: де\n"
     ]
    }
   ],
   "source": [
    "with open(\"GramEval2020-GSD-train.conllu\", \"r\", encoding=\"utf-8\") as data_file:\n",
    "    sentences = {}\n",
    "\n",
    "    parser = parse_incr(data_file)\n",
    "    for i, sentence in enumerate(parser):\n",
    "        # if i != 0:\n",
    "        #     break\n",
    "        for token in sentence:\n",
    "            if token['deprel'] == 'dislocated' or token['deprel'] == 'discourse':\n",
    "                collected_sentence = collect_sentence(sentence)\n",
    "                sentences[collected_sentence] = (token[\"form\"], token['deprel'])\n",
    "                # break чтобы одно предложение с 2+ токенами появлялось лишь однажды\n",
    "                break\n",
    "\n",
    "if len(sentences) == 0:\n",
    "    print(\"Тегов dislocated и discourse нет в файле\")\n",
    "else:\n",
    "    for k, v in sentences.items():\n",
    "        print(f\"{k}\\nСлово с тегом {v[1]}: {v[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dd975c-1922-4d32-833e-420e75a0f78a",
   "metadata": {},
   "source": [
    "#### Задача 4.\n",
    "\n",
    "Возьмите любой достаточно длинный (лучше новостной) текст. Любым известным инструментом извлеките именованные сущности из этого текста и выведите их списком по категориям (т.е. персоны вместе, локации вместе, организации вместе). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6b6f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsNERTagger,\n",
    "    MorphVocab,\n",
    "    Doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb806a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Места\n",
      "Балтийского Морить, Пальмира, Туапсе, Великобритания, Крым, Тюменской Область, Америка, Земля, Пермь, Амстердам, Техас, Украина, Китай, Карелия, Британия, Капитолий, Чукотка, Якутия, Афганистан, Хабаровскому Крать, Ссср, Краснокаменка, Донбасс, Феодосия, Шереметьево, Япония, Литва, Кубань, Ростовскую Область, Северной Америк, Нидерланды, Ялта, Россия, Берлин, Кабул, Сша, Фрг, Франция, Суэцком Канал, Весна, Ирак, Белоруссия, Геленджик, Суэцкого Канал, Польша, Москва, Норильск, Токио, Германия, Сочи, Австралия, Варшава, Казань, Юг, Саратовской Областить, Старомарьевка, Санкт-Петербург, Краснодарский Край, Евросоюз, Европа, Днр, Штат, Соединённых Штатах Америк, Иркутский, Лнр, \n",
      "Люди\n",
      "Николя Саркози, Рашкина, Казаться, Денис Майдан, Александр Бастрыкин, Тимофей Бажен, Елены Агапов, Антониу Гутерреш, Петр Толстой, Муратов, Джо Байден, Элла Памфилов, Юлией Пересильда, Дмитрий Певцов, Меркель, Мария Захаров, Евгений Зиничев, Валентина Матвиенко, Алексей Навальный, Владимира Путин, Фрау Меркель, Ольги Овчинников, Анна Кузнецов, Франк-Вальтер Штайнмайер, Александр Хинштейн, Дмитрий Мурат, Валерий Рашкин, Рустама Бигильдин, Пересильда, Мария Ресс, Ашраф Гануть, Александра Мельник, Владимир Путин, Саркози, Дональда Трамп, Петра Чайковский, Александр Лукашенко, Денис Пушилин, Маргарита Симоньян, Сангаджи Тарбай, Барака Обам, Вячеслав Володин, Зиничев, Евгению Зиничев, Михаила Горбачёв, Трамп, Ангелу Меркель, Матвиенко, Валерия Рашкин, Ильназ Галявий, Анатолий Вассерман, Климом Шипенко, Байден, Никиты Панфилов, \n",
      "Организации\n",
      "Исдм-Рослесхоз, Всу, Фонд Борьбы С Коррупция, Пермского Государственного Национального Исследовательского Университет, Юнидо, Google, Youtube, Нобелевский Комитет, Совета Федерация, Следственного Комитет, Новой Газета, Мид, Женского Делового Альянс, Кпрф, Всемирного Банк, Брикс, Справедливой Россия, Twitter, Талибан, Сми, It-Компания, Штабы Навальный, Соцсеть, Team Spirit, Россия Сегодний, Госдума, Рослесхоз, Цик, Оон, Whatsapp, Фсин, Тасс, Генпрокуратура, Вооруженные Сила, Конгресс, Фонд Защиты Прав Гражданин, It-Гигант, Meta, Олимпийского Комитета Россия, Оксфордского Словарить, Международной Организации По Миграция, Facebook, Апелляционный Суд, Ив Рош, Женской Двадцатка, Единая Россия, Лдпр, Единой Россия, Парламентской Газета, Роскомнадзор, Международной Космической Станция, Институт Экспериментальной Медицин, Zoom, Государственной Дум, Справедливая Россия — За Правда, Мчс, Квн, "
     ]
    }
   ],
   "source": [
    "segmenter = Segmenter()\n",
    "embeding = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(embeding)\n",
    "morph_vocab = MorphVocab()\n",
    "ner_tagger = NewsNERTagger(embeding)\n",
    "\n",
    "with open(\"новости.txt\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "doc = Doc(text)\n",
    "doc.segment(segmenter)\n",
    "doc.tag_ner(ner_tagger)\n",
    "\n",
    "locations, organisations, persons = [], [], []\n",
    "\n",
    "for span in doc.spans:\n",
    "    if span.type == \"LOC\":\n",
    "        locations.append(span.text)\n",
    "    elif span.type == \"PER\":\n",
    "        persons.append(span.text)\n",
    "    else:\n",
    "        organisations.append(span.text)\n",
    "\n",
    "#лемматизируем, чтобы все было в н.ф.\n",
    "locations = [morph_vocab.normal_forms(word)[0].title() for word in locations]\n",
    "persons = [morph_vocab.normal_forms(word)[0].title() for word in persons]\n",
    "organisations = [morph_vocab.normal_forms(word)[0].title() for word in organisations]\n",
    "\n",
    "locations = set(locations)\n",
    "persons = set(persons)\n",
    "organisations = set(organisations)\n",
    "\n",
    "# хотя при лемматизации у нас получается\n",
    "# Балтийского Морить и Фонд Борьбы С Коррупция\n",
    "# тут надо что-то посложнее, чем просто .normal_forms\n",
    "# или забить и максимум превратить списки в множества\n",
    "\n",
    "print(\"Места\")\n",
    "for i in locations:\n",
    "    print(i, end=\", \")\n",
    "\n",
    "print(\"\\nЛюди\")\n",
    "for i in persons:\n",
    "    print(i, end=\", \")\n",
    "\n",
    "print(\"\\nОрганизации\")\n",
    "for i in organisations:\n",
    "    print(i, end=\", \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add27567-eca5-4515-8671-0cd08c220e8e",
   "metadata": {},
   "source": [
    "#### Задача 5 (3 балла). \n",
    "\n",
    "А. Циммерлинг в [статье 2023 года](https://dialogue-conf.org/media/5937/zimmerlingav120.pdf) исследовал дативно-предикативные конструкции (вида \"мне холодно\", \"мне больно\" и т.п.). Возьмите любой достаточно большой датасет (можно попросить у меня корпус ГИКРЯ или создать собственный) и посчитайте аналогичные метрики на этом датасете для предикатов, которые приведены в статье. Имейте в виду, что тега \"предикатив\" в UD нет, потому и предлагается конкретный список слов, но слова типа \"холодно\"/\"хорошо\" могут не быть предикатами. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0798e12c-e771-45d9-b361-035fab13225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
